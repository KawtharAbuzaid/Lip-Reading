{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and Normalizing the Arabic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "input_dir = \"/lip reading/pretrain\"\n",
    "\n",
    "#preprocessing functions\n",
    "def clean_txt(input_str):\n",
    "    try:\n",
    "        if input_str:\n",
    "            input_str = re.sub('[?؟!@#$%&*+~\\/=><،.؛!]+^', '', input_str)\n",
    "            input_str = re.sub(r'[a-zA-Z?]', '', input_str).strip()\n",
    "            input_str = re.sub('[\\\\s]+', \" \", input_str)\n",
    "            input_str = re.sub(r'\\d', '', input_str).strip()\n",
    "            input_str = re.sub(r'[٠-٩]', '', input_str).strip()\n",
    "            input_str = re.sub(r\" ?\\([^)]+\\)\", \"\", input_str)\n",
    "            input_str = input_str.replace(\"؟\", ' ').replace(\"!\", ' ').replace(\"/\", ' ')\n",
    "            input_str = input_str.replace(\",\", '').replace(\".\", '').replace(\":\", ' ')\n",
    "            input_str = input_str.strip()\n",
    "    except:\n",
    "        return input_str\n",
    "    return input_str\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)  # Normalize ى to ي first\n",
    "    text = re.sub(r\"ي\\b\", \"ى\", text)  # Replace ي with ى only at the end of words\n",
    "    text = re.sub(\"ؤ\", \"و\", text)\n",
    "    text = re.sub(\"ئ\", \"ي\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "\n",
    "    cleaned_content = clean_txt(content)\n",
    "    normalized_content = normalize_arabic(cleaned_content)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(normalized_content)\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            preprocess_text_file(file_path)\n",
    "\n",
    "print(f\"Text preprocessing complete! All .txt files in {input_dir} have been updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "organizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shutil\n",
    "\n",
    "\n",
    "input_dir = \"/lip reading/pretrain\"  \n",
    "output_dir = \"/lip reading/cleanD\" \n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    npy_files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        \n",
    "        txt_file_path = os.path.join(folder_path, txt_file)\n",
    "        with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "            class_name = file.read().strip()\n",
    "\n",
    "        class_folder = os.path.join(output_dir, class_name)\n",
    "        os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "        for npy_file in npy_files:\n",
    "            if npy_file.startswith(os.path.splitext(txt_file)[0]):  \n",
    "                src_npy_path = os.path.join(folder_path, npy_file)\n",
    "                dest_npy_path = os.path.join(class_folder, npy_file)\n",
    "                shutil.copy(src_npy_path, dest_npy_path)  \n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    if any(f.endswith('.txt') for f in files):\n",
    "        process_folder(root)\n",
    "\n",
    "print(f\"Dataset processing complete! All files are organized in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Output directory where the dataset is created\n",
    "output_dir = \"/lip reading/cleanD\"\n",
    "\n",
    "# Get the list of class folders\n",
    "class_folders = [folder for folder in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, folder))]\n",
    "\n",
    "# Print the total number of classes and their names\n",
    "print(f\"Total number of classes: {len(class_folders)}\")\n",
    "print(\"Class names:\")\n",
    "for class_name in class_folders:\n",
    "    print(f\"- {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"/lip reading/cleanD\"\n",
    "classes = os.listdir(main_dir)  # List all class folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the maximum shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_shape = None\n",
    "\n",
    "# Step 1: Identify the maximum shape among all .npy files\n",
    "for class_dir in class_to_idx:\n",
    "    class_path = os.path.join(main_dir, class_dir)\n",
    "    if os.path.isdir(class_path):  # Ensure it's a directory\n",
    "        for npy_file in os.listdir(class_path):\n",
    "            if npy_file.endswith('.npy'):\n",
    "                file_path = os.path.join(class_path, npy_file)\n",
    "                data = np.load(file_path)\n",
    "                if max_shape is None:\n",
    "                    max_shape = data.shape\n",
    "                else:\n",
    "                    max_shape = tuple(max(max_shape[dim], data.shape[dim]) for dim in range(len(data.shape)))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Padding the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Pad files to the maximum shape\n",
    "for class_dir in class_to_idx:\n",
    "    class_path = os.path.join(main_dir, class_dir)\n",
    "    if os.path.isdir(class_path):  # Ensure it's a directory\n",
    "        for npy_file in os.listdir(class_path):\n",
    "            if npy_file.endswith('.npy'):\n",
    "                file_path = os.path.join(class_path, npy_file)\n",
    "                data = np.load(file_path)\n",
    "\n",
    "                # Calculate padding sizes\n",
    "                padding = [(0, max_dim - curr_dim) for curr_dim, max_dim in zip(data.shape, max_shape)]\n",
    "\n",
    "                # Apply padding\n",
    "                padded_data = np.pad(data, pad_width=padding, mode='constant', constant_values=0)\n",
    "\n",
    "                # Save the padded file (overwrite or save to a new location)\n",
    "                np.save(file_path, padded_data)\n",
    "\n",
    "print(\"Padding completed. All files have the shape:\", max_shape)\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
